{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-Soke/FAW-AI-BINARY-MODEL/blob/main/ONNX_export.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDOcOpyigSkm",
        "outputId": "11ff13e0-1299-4e44-a15e-ca8dd8a1db72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'FAW-AI-BINARY-MODEL'...\n",
            "remote: Enumerating objects: 1387, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 1387 (delta 12), reused 0 (delta 0), pack-reused 1359 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1387/1387), 181.65 MiB | 18.54 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n",
            "/content/FAW-AI-BINARY-MODEL\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/the-Soke/FAW-AI-BINARY-MODEL.git\n",
        "%cd FAW-AI-BINARY-MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "n6WMnOaBgfHq",
        "outputId": "2faaf9f8-c45e-4e37-8043-6fe6cd03e2fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e734827e-ae0e-4dd0-a392-8674cfd1222f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e734827e-ae0e-4dd0-a392-8674cfd1222f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving mymodel (1).pt to mymodel (1).pt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "640ba532",
        "outputId": "e378c865-cf3a-4a1c-bdb1-61d4bc6d3515"
      },
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "notebook_path = 'PIPELINE_METRICS.ipynb'\n",
        "\n",
        "try:\n",
        "    with open(notebook_path, 'r') as f:\n",
        "        notebook_content = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The notebook '{notebook_path}' was not found.\")\n",
        "    notebook_content = None\n",
        "\n",
        "found_classes = []\n",
        "if notebook_content:\n",
        "    for cell in notebook_content['cells']:\n",
        "        if cell['cell_type'] == 'code':\n",
        "            source = \"\".join(cell['source'])\n",
        "            for match in re.finditer(r'class\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*(\\(.*?\\))?\\s*:', source):\n",
        "                class_name = match.group(1)\n",
        "                inheritance = match.group(2) if match.group(2) else ''\n",
        "                found_classes.append(f\"{class_name}{inheritance}\")\n",
        "\n",
        "if found_classes:\n",
        "    print(\"Found the following class definitions in 'PIPELINE_METRICS.ipynb':\")\n",
        "    for cls in found_classes:\n",
        "        print(cls)\n",
        "    print(\"\\nPlease identify which of these is your model class.\")\n",
        "else:\n",
        "    print(f\"No class definitions found in '{notebook_path}'.\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No class definitions found in 'PIPELINE_METRICS.ipynb'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7ae8ad4",
        "outputId": "973612f0-76fd-416a-d741-cd22883b7434"
      },
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "notebook_path = 'PIPELINE.ipynb'\n",
        "\n",
        "try:\n",
        "    with open(notebook_path, 'r') as f:\n",
        "        notebook_content = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The notebook '{notebook_path}' was not found.\")\n",
        "    notebook_content = None\n",
        "\n",
        "found_classes = []\n",
        "if notebook_content:\n",
        "    for cell in notebook_content['cells']:\n",
        "        if cell['cell_type'] == 'code':\n",
        "            source = \"\".join(cell['source'])\n",
        "            # This regex captures class names that might inherit from other classes or simple class definitions\n",
        "            for match in re.finditer(r'class\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*(\\(.*?\\))?\\s*:', source):\n",
        "                class_name = match.group(1)\n",
        "                inheritance = match.group(2) if match.group(2) else ''\n",
        "                found_classes.append(f\"{class_name}{inheritance}\")\n",
        "\n",
        "if found_classes:\n",
        "    print(\"Found the following class definitions in 'PIPELINE.ipynb':\")\n",
        "    for cls in found_classes:\n",
        "        print(cls)\n",
        "    print(\"\\nPlease identify which of these is your model class (e.g., FAWNet or the one associated with resnet18_faw_model).\")\n",
        "else:\n",
        "    print(f\"No class definitions found in '{notebook_path}'.\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No class definitions found in 'PIPELINE.ipynb'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c07d95",
        "outputId": "412fb5cb-e454-4fb5-ee3b-2111df9ef504"
      },
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "notebook_path = 'PIPELINE.ipynb'\n",
        "\n",
        "try:\n",
        "    with open(notebook_path, 'r') as f:\n",
        "        notebook_content = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The notebook '{notebook_path}' was not found.\")\n",
        "    notebook_content = None\n",
        "\n",
        "if notebook_content:\n",
        "    model_class_found = False\n",
        "    for cell in notebook_content['cells']:\n",
        "        if cell['cell_type'] == 'code':\n",
        "            source = \"\".join(cell['source'])\n",
        "            # Search for class definitions\n",
        "            # This regex captures class names that might inherit from other classes (e.g., class MyModel(nn.Module):)\n",
        "            # or simple class definitions (e.g., class MyModel:)\n",
        "            for match in re.finditer(r'class\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*(\\(.*?\\))?\\s*:', source):\n",
        "                class_name = match.group(1)\n",
        "                inheritance = match.group(2) if match.group(2) else ''\n",
        "                print(f\"Found class definition: {class_name}{inheritance}\")\n",
        "                # Heuristic to identify potential PyTorch model classes\n",
        "                if \"torch.nn.Module\" in inheritance or \"nn.Module\" in inheritance or \"FAWNet\" in class_name:\n",
        "                    print(f\"Potential model class: {class_name}\")\n",
        "                    model_class_found = True\n",
        "                elif re.search(r'super\\(.*?,\\s*self\\)\\.__init__\\(\\)', source): # Check for super().__init__() call\n",
        "                    print(f\"Potential model class (with super().__init__): {class_name}\")\n",
        "                    model_class_found = True\n",
        "\n",
        "    if not model_class_found:\n",
        "        print(f\"No obvious model class (like FAWNet or inheriting from nn.Module) found in '{notebook_path}'. You might need to define your model architecture.\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No obvious model class (like FAWNet or inheriting from nn.Module) found in 'PIPELINE.ipynb'. You might need to define your model architecture.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Step 1 — Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Step 2 — Recreate the architecture\n",
        "model = models.resnet18(weights=None)  # same architecture\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)  # binary classifier\n",
        "\n",
        "# Step 3 — Load the trained weights\n",
        "model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Model loaded and ready for export\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvZZWLHfrM1E",
        "outputId": "f6fd8504-5108-4a46-800e-032afefc2685"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded and ready for export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SisaixY0rnz3",
        "outputId": "a2a29f71-563b-44b6-8f4b-b569c438acd9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 3, 224, 224, device=device)\n"
      ],
      "metadata": {
        "id": "50Wq0dtFsDht"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "\n",
        "onnx_path = \"model.onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,                            # model being run\n",
        "    dummy_input,                      # model input\n",
        "    onnx_path,                        # output path\n",
        "    export_params=True,               # store trained weights inside model file\n",
        "    opset_version=11,                 # ONNX version\n",
        "    do_constant_folding=True,         # optimize constants\n",
        "    input_names=['input'],            # name of the model’s input\n",
        "    output_names=['output'],          # name of the model’s output\n",
        "    dynamic_axes={                    # allow variable batch sizes\n",
        "        'input': {0: 'batch_size'},\n",
        "        'output': {0: 'batch_size'}\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"✅ Model exported successfully to {onnx_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWzleiROsJF7",
        "outputId": "c37d3b2d-fd56-462c-907e-5a334f7d9719"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.19.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2526556278.py:5: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model exported successfully to model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "onnx_model = onnx.load(\"model.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"✅ ONNX model is valid and ready to use!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iwlBF_Ks8Ou",
        "outputId": "ee8b0a74-bfe3-4783-c30f-c389c5af75dd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ONNX model is valid and ready to use!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING"
      ],
      "metadata": {
        "id": "3Oq7nIzhtHJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnx onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9d_olc4tI15",
        "outputId": "50b3fe79-1e65-4b91-bc21-bac28b79a223"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.19.1)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# Create a runtime session\n",
        "session = ort.InferenceSession(\"model.onnx\")\n",
        "\n",
        "# Print input details (names, shapes, etc.)\n",
        "for i in session.get_inputs():\n",
        "    print(f\"Input: {i.name}, shape: {i.shape}, type: {i.type}\")\n",
        "\n",
        "# Prepare dummy input (use same shape as training input, e.g. 3x224x224 for ResNet)\n",
        "dummy_input = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
        "\n",
        "# Run inference\n",
        "outputs = session.run(None, {\"input\": dummy_input})\n",
        "\n",
        "print(\"✅ Inference completed!\")\n",
        "print(\"Output shape:\", outputs[0].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaJohhEUtXUt",
        "outputId": "2de6b90d-b125-4a7a-bf1b-cd6c0f251134"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: input, shape: ['batch_size', 3, 224, 224], type: tensor(float)\n",
            "✅ Inference completed!\n",
            "Output shape: (1, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PUSHING"
      ],
      "metadata": {
        "id": "9QgHvTl9tpyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd path/to/FAW-AI-BINARY-MODEL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cp_bsdFtrLS",
        "outputId": "7d31d00a-2673-440c-fb87-b3e9b99349ce"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'path/to/FAW-AI-BINARY-MODEL'\n",
            "/content/FAW-AI-BINARY-MODEL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"kesibosoke@gmail.com\"\n",
        "!git config --global user.name \"the-Soke\""
      ],
      "metadata": {
        "id": "wum0BCsHug-X"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"exported the ONNX file\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UazNmrkUuibU",
        "outputId": "a53a79e1-e64a-4703-c2f9-d74f4bfaeb82"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 97d915d] exported the ONNX file\n",
            " 2 files changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 model.onnx\n",
            " create mode 100644 model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOSjfsi7u9sQ",
        "outputId": "af3b8641-2878-49fa-b661-520adeb0096a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 5, done.\n",
            "Counting objects:  20% (1/5)\rCounting objects:  40% (2/5)\rCounting objects:  60% (3/5)\rCounting objects:  80% (4/5)\rCounting objects: 100% (5/5)\rCounting objects: 100% (5/5), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 79.24 MiB | 8.03 MiB/s, done.\n",
            "Total 4 (delta 0), reused 0 (delta 0), pack-reused 0\n",
            "To https://github.com/the-Soke/FAW-AI-BINARY-MODEL.git\n",
            "   3d456e7..97d915d  main -> main\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6s42BIp/IW+cvH/RIGDtP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}